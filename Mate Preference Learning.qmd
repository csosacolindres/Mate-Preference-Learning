---
title: "How To Learn Your Mate Preferences"
format: html
editor: visual
---

## Mate Preference Learning

**Purpose**: This model was designed to explore how might species-typical mating psychology give rise to individually tailored mate preferences. Using an agent-based model, we will test the ability of a reinforcement learning algorithm to reproduce the ideal mate preferences of N = 1,044 participants.

âœ… **Agents**: Each agent represents a participant. The agents are made up of 15 traits, an initial preference vector (10's across all traits), an initial reward vector (date mate value given preferences), memory for reward vector, memory for preference vector.

**Life Cycle:**

1.  Mate choice for n cycles using preference vector.

    1.  Each cycle, they choose a mate that maximizes mv\*received/sent

    2.  Store mv\*received/sent in memory for reward vector (needs to be constantly recalc)

    3.  Preference vector goes into preference memory

2.  Change preferences relative to memory by methods:

    1.  Random

    2.  Decreased fixed amount (need to code diff for this)

    3.  Move towards highest reward partner (and this)

3.  Mate choice for n cycles using changed preference vector

    1.  Compare max reward to max reward stored in memory (do new preferences do better than old preferences?)

        1.  If better: Store new preferences in memory & store reward in memory

        2.  If not better: Keep memory unchanged; discard new preference & reward vector.

4.  Cycle back to #2 (change preferences relative to memory by methods---for now only Random) until preferences are no longer changing.

Parameters

```{r}
#PARAMETERS

# Starting Values for Prefernces, Reward Memory. 
startPref <- 10
startRMem <- 0

# Interest Ratio (starting values)
startRecieved <- 1
startSent <- 1

# Cycles 
prefLearningCycles <- 10000
mateChoiceCycles <- 50

```

Agent Generation

```{r}


# Human_data; PIN(3); sex(4); Ideal(235:249) Affectionate, Ambition, Artistic, Disposition, Family, Health, Humor, Intelligent, Kind, Parenting, Physical Attractiveness, Religious, Resources, Sexy, Status; Importance(283:297) " ; Self Composite(345:359) " ; Self Mate Value Composite(418) "

data <- read.csv("Human_data.csv")
# Sampled 10 people to check if things are working right. 
data <- data[c(1:5, 950:954),]

males <- data[data$sex == 1,]
females <- data[data$sex == 0,]


# MALES: TRAITS ; PREFERENCE VECTOR, MEMEORY ; REWARD VECTOR, MEMORY
mTraits <- males[, c(345:359)]
mPref <- matrix(startPref, nrow(males), 15)
colnames(mPref) <- c("prefVector_affectionate", 
                            "prefVector_ambition", 
                            "prefVector_artistic", 
                            "prefVector_disposition", 
                            "prefVector_family", 
                            "prefVector_health", 
                            "prefVector_humor", 
                            "prefVector_intelligent", 
                            "prefVector_kind", 
                            "prefVector_parenting", 
                            "prefVector_physAtt", 
                            "prefVector_religious", 
                            "prefVector_resources", 
                            "prefVector_sexy", 
                            "prefVector_status")
mPrefMemory <- matrix(NA, nrow(males), 15)
colnames(mPrefMemory) <- c("prefMemory_affectionate", 
                            "prefMemory_ambition", 
                            "prefMemory_artistic", 
                            "prefMemory_disposition", 
                            "prefMemory_family", 
                            "prefMemory_health", 
                            "prefMemory_humor", 
                            "prefMemory_intelligent", 
                            "prefMemory_kind", 
                            "prefMemory_parenting", 
                            "prefMemory_physAtt", 
                            "prefMemory_religious", 
                            "prefMemory_resources", 
                            "prefMemory_sexy", 
                            "prefMemory_status")

mRewMemory <- matrix(startRMem, nrow(males), nrow(females))






# FEMALES: PREFERENCE VECTOR, MEMORY ; REWARD VECTOR, MEMORY
fTraits <- females[, c(345:359)]
fPref <- matrix(startPref, nrow(females), 15)
colnames(fPref) <- c("prefVector_affectionate", 
                            "prefVector_ambition", 
                            "prefVector_artistic", 
                            "prefVector_disposition", 
                            "prefVector_family", 
                            "prefVector_health", 
                            "prefVector_humor", 
                            "prefVector_intelligent", 
                            "prefVector_kind", 
                            "prefVector_parenting", 
                            "prefVector_physAtt", 
                            "prefVector_religious", 
                            "prefVector_resources", 
                            "prefVector_sexy", 
                            "prefVector_status")

fPrefMemory <- matrix(NA, nrow(females), 15)
colnames(fPrefMemory) <- c("prefMemory_affectionate", 
                            "prefMemory_ambition", 
                            "prefMemory_artistic", 
                            "prefMemory_disposition", 
                            "prefMemory_family", 
                            "prefMemory_health", 
                            "prefMemory_humor", 
                            "prefMemory_intelligent", 
                            "prefMemory_kind", 
                            "prefMemory_parenting", 
                            "prefMemory_physAtt", 
                            "prefMemory_religious", 
                            "prefMemory_resources", 
                            "prefMemory_sexy", 
                            "prefMemory_status")

fRewMemory <- matrix(startRMem, nrow(females), nrow(males))


# Putting the AGENTS together. Make traits, preferences, and preference memory as a part of the agent. The ENVIORNMENT is the rewards, reward memory, and the choices.
mAgents <- cbind(mTraits, mPref, mPrefMemory)
fAgents <- cbind(fTraits, fPref, fPrefMemory)




```

Functions

```{r}

# Mate Value Calculation : own pref vector, others trait composite
mvcalc <- function(pref, traits){
  names(pref) <- 1:15
  names(traits) <- 1:15
  mv <- 10 *(-1 *(dist(rbind(pref, traits))) + sqrt(10^2 *15)) / sqrt(10^2 *15)
  return(mv)
}

# Reward Calculation : interest recieved, interest sent
rcalc <- function(mv, recieved, sent) {
  reward <- mv * recieved / sent
  return(reward)
}

```

SIMULATION

Learning Cycles:

-   change preferences

-   calculate MV

-   create choice matrix

-   mate choice cycles

    -   calculate reward

    -   choose mate

    -   update choice matrix

-   compare reward

update as necessary

Take this models reward as is (before update preferences). Save to memory.

Copy and paste to for loop that has (new) ability to change preferences.

Random Update Methods

```{r}

# make randomMethod only take one prefernce at a time... on second thought, this could be controlled by parameter; randomly change n preferences at a time. 

# Random Method: Change row values for every col to a random number. Don't change highest reward one. 
# Base Random Method
randomMethod <- function(pref){
  
  random <- t(apply(pref, 1, function(y) runif(y, min = 0, max = 10)))
  colnames(random) <- colnames(pref)
  pref <- random
  
  
  return(pref)
}
  
# How do I make this so it changes prefernces n at a time? And how do I make it so they change least if most important, and most if least important? 



```

Check if this is just anchoring to mvs and randomly changing prefs to = mvs

```{r}

# if (first not first generation, Change Preferences: )

for(p in 1:prefLearningCycles){
  
  # make 2 randomMethods calc ( for male AND females)
  # Overide preferences save as updates come. 
  if(p > 1) {
    mPref <- randomMethod(mPref)
    fPref <- randomMethod(fPref)
  }
  
  # Calculate MV
  mMVs <- t(apply(mPref, 1, function(x) apply(fTraits, 1, function(y) mvcalc(x, y))))
  fMVs <- t(apply(fPref, 1, function(x) apply(mTraits, 1, function (y) mvcalc(x, y))))
  
  # Create choice matrix:
  mChoice <- matrix(1, nrow(males), nrow(females))
  fChoice <- matrix(1, nrow(females), nrow(males))
  
  
  for(c in 1:mateChoiceCycles){
    
    # Calculate Reward 
    mReward <- rcalc(mMVs, fChoice, mChoice)
    fReward <- rcalc(fMVs, mChoice, fChoice)
    
    # Choose Mate 
    mMaxReward <- cbind(1:nrow(males), apply(mReward, 1, which.max))
    fMaxReward <- cbind(1:nrow(females), apply(fReward, 1, which.max))
    
    
    
    # Update Choice Matrices
    mChoice[mMaxReward] <- mChoice[mMaxReward] + 1
    fChoice[fMaxReward] <- fChoice[fMaxReward] + 1
    
    
  
  }
  
  # Commit to Memory 
  # Store mReward & fReward in Reward Memory (of MaxReward mates?) and compare vector to memory
  for(m in 1:nrow(males)){
    maxNow <- max(mReward[m, ])
    maxMemory <- max(mRewMemory[m, ])
      
    if(maxNow >= maxMemory){
      mRewMemory[m, ] <- mReward[m, ]
      mPrefMemory[m, ] <- mPref[m, ]
      }
    }
  
  for(f in 1:nrow(females)){
    maxNow <- max(fReward[f, ])
    maxMemory <- max(fRewMemory[f, ])
    
    if(maxNow >= maxMemory){
      fRewMemory[f, ] <- fReward[f, ]
      fPrefMemory[f, ] <- fPref[f, ]
      }
    }
}


# Correlation between learned prefernces and real prefernces. 


# Need to standardize outcome and predictor varibles 
# multivariet model withere i have 15 oucome variables . Later on . 


```

First sentence ; organisms have preferences that guide them towards mates.

```{r}
# Library
library(rstanarm)
library(tidyr)
```

```{r}

##### ANALYSIS #####


# Transform Data #

mPrefMemory <- rowid_to_column(as.data.frame(mPrefMemory), "id")
mPrefMemory <- mPrefMemory |> pivot_longer(-id, names_to = "trait", names_prefix = "prefMemory_", values_to = "predicted_value")


observed <- males[, 235:249] |> as.data.frame() |> rowid_to_column("id")
observed <- observed |> pivot_longer(-id, names_to = "trait", names_prefix = "ideal_", values_to = "observed_value")


d <- left_join(mPrefMemory, observed, by = c("id", "trait"))
d_z <- d |> group_by(trait) |> mutate(predicted_value = (predicted_value - mean(observed_value))/observed_value, observed_value = (observed_value - mean(observed_value))/observed_value) %>% ungroup()


# Multilevel Model #

fit <- stan_glmer(predicted_value ~ observed_value + (1|id), data = d_z)
fitSummary <- summary(fit)

```
