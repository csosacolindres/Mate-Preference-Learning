---
title: "How To Learn Your Mate Preferences"
format: html
editor: visual
---

## Mate Preference Learning

**Purpose**: This model was designed to explore how might species-typical mating psychology give rise to individually tailored mate preferences. Using an agent-based model, we will test the ability of a reinforcement learning algorithm to reproduce the ideal mate preferences of N = 1,044 participants.

âœ… **Agents**: Each agent represents a participant. The agents are made up of 15 traits, an initial preference vector (10's across all traits), an initial reward vector (date mate value given preferences), memory for reward vector, memory for preference vector.

**Life Cycle:**

1.  Mate choice for n cycles using preference vector.

    1.  Each cycle, they choose a mate that maximizes mv\*received/sent

    2.  Store mv\*received/sent in memory for reward vector (needs to be constantly recalc)

    3.  Preference vector goes into preference memory

2.  Change preferences relative to memory by methods:

    1.  Random

    2.  Decreased fixed amount (need to code diff for this)

    3.  Move towards highest reward partner (and this)

3.  Mate choice for n cycles using changed preference vector

    1.  Compare max reward to max reward stored in memory (do new preferences do better than old preferences?)

        1.  If better: Store new preferences in memory & store reward in memory

        2.  If not better: Keep memory unchanged; discard new preference & reward vector.

4.  Cycle back to #2 (change preferences relative to memory by methods---for now only Random) until preferences are no longer changing.

```{r packages}

library(ggplot2)
library(ggridges)
library(data.table)
```

Data

```{r data}

#Load the human data
data <- read.csv("Human_data.csv")
data <- data[complete.cases(data[, c(6, 235:249, 345:359, 283:297, 299:213)]),]
data <- data[data$CIN %in% data$CIN[duplicated(data$CIN)],]
```

Parameters

```{r parameters}

# the 15 Traits
traitNames <- gsub("ideal_","",colnames(data[,235:249]))


# Starting Values for Prefernces and Reward Memory. 
startPref <- 10
startRMem <- 0


# number of males and females
# (optionally set to a lower number for testing)
maleNum <- as.numeric(table(data$sex)[2])
femaleNum <- as.numeric(table(data$sex)[1])


# All Methods
methodList <- c("randRand", "randConstant", "impConstant", "randImp", "impImp", "allImp", "allImpBi")


# Amount of change for constant methods
changeConstant <- c(NA,rep(.1,2),rep(.5,4))


# probability of change going down vs up 
downProb <- c(rep(NA,6),.8)


# Cycles 
prefLearningCycles <- rep(150, 7)
prefTestingCycles <- rep(maleNum*3, 7)


#Combine all parameter settings into a matrix for looping
pMatrix<-data.frame("method"=methodList,
                    "changeConstant"=changeConstant,
                    "downProb"=downProb,
                    "prefLearningCycles"=prefLearningCycles,
                    "prefTestingCycles"=prefTestingCycles)


#Number of times to repeat each model
modelLoops<- 5

```

Functions

```{r}

# Mate Value Calculation : own pref vector, others trait composite
mvcalc <- function(pref, traits){
  names(pref) <- 1:15
  names(traits) <- 1:15
  mv <- 10 *(-1 *(dist(rbind(pref, traits))) + sqrt(10^2 *15)) / sqrt(10^2 *15)
  
  return(mv)
}



# Reward Calculation : interest recieved / interest sent 
rcalc <- function(mv, recieved, sent) {
  
  interestRatio <- t(recieved) / sent
  interestRatio[interestRatio>1.25]<-1.25
  reward <- mv * interestRatio 
  
  return(reward)
}




mpUpdate <- function(method = "method", pref, imp, changeConstant, downProb) {
  
  if(method == "randRand") {
    
    # Random/Random: Selects 1 random trait to change. Changes selected trait to random mate preference value.
    
      random <- t(apply(pref, 1, function(y) 
        round(runif(y, 
                    min = 0, 
                    max = 10), 1)))
      colnames(random) <- colnames(pref)
      
      changeMat <- cbind(1:nrow(pref),
                         sample(1:15, 
                                nrow(pref), 
                                replace = T))
      
      pref[changeMat] <- random[changeMat]
    
    } 
  
  
  
  if(method == "randConstant") {
      
      # Random/Constant: Selects all trait to change. Changes traits down by changeConstant amount.
    
      new <- pref - changeConstant 
      colnames(new) <- colnames(pref)
      
      changeMat <- cbind(1:nrow(pref), 
                         sample(1:15, 
                                nrow(pref),
                                replace = T))
      
      pref[changeMat] <- new[changeMat]
    
    }
  

  
  if(method == "impConstant") {
    
    # Imp/Constant: Selects a trait to change proportional to how important the traitt is to the specific participant. Changes selected trait down by changeConstant.  
    
      new <- pref - changeConstant 
      colnames(new) <- colnames(pref)
      
      changeVec <- apply(imp, 1, function(x) 
        sample(1:15, 1, prob = 11 - x))
      
      changeMat <- cbind(1:nrow(pref), changeVec)
      
      pref[changeMat] <- new[changeMat]
    
    }
    
  
  
  if(method == "allImp"){
        
    # All/Imp: Selects all traits. Changes values down by mag (a value proportional to importance with a max amount that preference value can change). More important traits go down less amount. Less important traits go down less amount.
    
    changeConstant <- ((11-imp)/11) * changeConstant
    
    pref <- pref - changeConstant
  }
  
  
  
  if(method == "randImp") {
    
        
    # Random/Imp: Selects 1 trait at random. Changes values down by mag (a value proportional to importance and a max amount that preference value decreses by). More important traits go down less amount. Less important traits go down less amount.
    
    changeConstant <- ((11-imp)/11) * changeConstant
    new <- pref - changeConstant
    
    changeVec <- apply(imp, 1, function(x) sample(1:15, 1))
    changeMat <- cbind(1:nrow(pref), changeVec)
    pref[changeMat] <- new[changeMat]
  
    }
  
  
  
  if(method == "impImp") {
  
    # Imp/Imp: Selects 1 trait proportional to how important the trait is (if more important, less likley to be picked. If less important, more likely to be picked). The the preference values decrease by mag (a value proportional to importance and a max amount that preference value decreases by). More important traits decrease less. Less important traits decrease more. 
    
    changeConstant <- ((11-imp)/11) * changeConstant
    new <- pref - changeConstant
    
    changeVec <- apply(imp, 1, function(x) 
      sample(1:15, 1, prob = 11 - x))
    
    changeMat <- cbind(1:nrow(pref), changeVec)
    pref[changeMat] <- new[changeMat]
    
    }
  
  
  
  if(method == "allImpBi"){
   
    # Magnitude Up-Down Method: Selects all traits. This can decrease and increase preference values. The traits increase or decrease depending on a weighted proportion (downProb). The preference values decrease by mag (a value proportional to importance and a max amount that preference value decreases by) where ore important traits decrease less. Less important traits decrease more. The preference values increase by mag where the more important traits increase more and the less important traits increase less. 
    
    if(rbinom(1, 1, prob = downProb)){
      
      changeConstant <- ((11-imp)/11) * changeConstant
      pref <- pref - changeConstant
      
    } else {
       
      changeConstant <- ((imp + 1)/11) * changeConstant
      
      pref <- pref + changeConstant
      pref[pref > 10] <- 10
       
    }
      
  }
  
 
  return(pref)
}


```

Agent Generation

```{r}

# seperating into sexes
females <- data[data$sex == 0, c(1,3:4,235:249,283:297,345:359)]
males <- data[data$sex == 1, c(1,3:4,235:249,283:297,345:359)]

# number of participants
females <- females[sample(1:nrow(females), femaleNum),]
males <- males[sample(1:nrow(males), maleNum),]

# participant's traits 
mTraits <- as.matrix(males[, c(34:48)])
fTraits <- as.matrix(females[, c(34:48)])

# participant's importance ratings 
fImp <- as.matrix(females[, c(19:33)])
mImp <- as.matrix(males[, c(19:33)])

```

# Simulation:

```{r}


# Create a dataframe for storing trial-by-trial correlations between learned and real preferences
trialCors <- data.frame(matrix(NA,1,8))
colnames(trialCors)<-c("o",colnames(pMatrix),"trial","r")

prefData<-data.frame(matrix(NA,1,54))
colnames(prefData)<-c("o",colnames(pMatrix),
                      colnames(females)[1:3],
                      paste0("obs_",traitNames),
                      paste0("imp_",traitNames),
                      paste0("learned_",traitNames))



#Loop each model m times
for (o in 1:modelLoops) {
  
  for (p in 1:nrow(pMatrix)) {
    # re-sets inital traits after a method; if not: old method pref updates will stay for next update method. I dont want that.
    
    fPref <- matrix(startPref, nrow(females), 15)
    colnames(fPref) <- traitNames
    
    fPrefMemory <- matrix(startPref, nrow(females), 15)
    colnames(fPrefMemory) <- traitNames
    
    fRewMemory <- matrix(startRMem, nrow(females), nrow(males))
    
    
    mPref <- matrix(startPref, nrow(males), 15)
    colnames(mPref) <- traitNames
    
    mPrefMemory <- matrix(startPref, nrow(males), 15)
    colnames(mPrefMemory) <- traitNames
    
    mRewMemory <- matrix(startRMem, nrow(males), nrow(females))
    
    
    # method cycled
    method <- pMatrix$method[p]
    
    
    for (l in 1:pMatrix$prefLearningCycles[p]) {
      # makes sure the right method is being used.
      if (l > 1) {
        fPref <- mpUpdate(
          pMatrix$method[p],
          pref = fPref,
          imp = fImp,
          changeConstant = pMatrix$changeConstant[p],
          downProb = pMatrix$downProb[p]
        )
        
        mPref <- mpUpdate(
          pMatrix$method[p],
          pref = mPref,
          imp = mImp,
          changeConstant = pMatrix$changeConstant[p],
          downProb = pMatrix$downProb[p]
        )
      }
      
      
      
      # Calculate MV
      fMV <- t(apply(fPref, 1, function(x)
        apply(mTraits, 1, function (y)
          mvcalc(x, y))))
      
      mMV <- t(apply(mPref, 1, function(x)
        apply(fTraits, 1, function(y)
          mvcalc(x, y))))
      
      
      
      # Create choice matrices:
      fChoice <- matrix(1, nrow(females), nrow(males))
      mChoice <- matrix(1, nrow(males), nrow(females))
      
      for (t in 1:pMatrix$prefTestingCycles[p]) {
        # Calculate Reward
        fReward <- rcalc(fMV, mChoice, fChoice)
        mReward <- rcalc(mMV, fChoice, mChoice)
        
        # Choose Mate
        fMaxReward <-
          cbind(1:nrow(females), apply(fReward, 1, which.max))
        mMaxReward <-
          cbind(1:nrow(males), apply(mReward, 1, which.max))
        
        # Update Choice Matrices
        fChoice[fMaxReward] <- fChoice[fMaxReward] + 1
        mChoice[mMaxReward] <- mChoice[mMaxReward] + 1
        
      }
      
      
      
      # Commit to Memory. If new reward is higher than old reward, replace old mate preference set with new mate preference set. If not, keep old mate preference set.
      for (f in 1:nrow(females)) {
        maxNow <- max(fReward[f,])
        maxMemory <- max(fRewMemory[f,])
        
        if (maxNow >= maxMemory) {
          fRewMemory[f,] <- fReward[f,]
          fPrefMemory[f,] <- fPref[f,]
          
        } else {
          fPref[f,] <- fPrefMemory[f,]
          
        }
        
      }
      
      
      for (m in 1:nrow(males)) {
        maxNow <- max(mReward[m,])
        maxMemory <- max(mRewMemory[m,])
        
        if (maxNow >= maxMemory) {
          mRewMemory[m,] <- mReward[m,]
          mPrefMemory[m,] <- as.numeric(mPref[m,])
          
        } else {
          mPref[m,] <- mPrefMemory[m,]
          
        }
        
      }
      
      
      #Compute the average correlation between learned and observed preferences
      
      r <- suppressWarnings(mean(diag(cor(rbind(fPref, mPref), 
                                          rbind(females[, 4:18],
                                                males[, 4:18])
                                          ))))
      
      r <- ifelse(is.na(r), 0, r)
      
      # Saves trial by trial correlations for each method being used.
      trialCors <- rbind(trialCors,
                         cbind(o, 
                               pMatrix[p, ],
                               "trial" = l,
                               r))
      
    }
    
    
    #Prepare learned preferences for saving
    fPref <- suppressWarnings(cbind(o, 
                             pMatrix[p, ],
                             females[, 1:33], 
                             fPref))
    
    mPref <- suppressWarnings(cbind(o, 
                                    pMatrix[p, ], 
                                    males[, 1:33], 
                                    mPref))
    
    colnames(fPref) <- colnames(prefData)
    colnames(mPref) <- colnames(prefData)
    
    prefData <- rbind(prefData, fPref, mPref)
    
  }
  
}

#Remove the unnecessary rows of NAs from the results dataframes
trialCors<-trialCors[-1,]
prefData<-prefData[-1,]

write.csv(trialCors, 
          file = paste0("TrialCorrelationResults", 
                        format(Sys.time(),"%Y%m%d %H%M%S"),".csv"),
          row.names=F)
write.csv(prefData, 
          file = paste0("PrefDataResults", 
                        format(Sys.time(),"%Y%m%d %H%M%S"), " .csv"),
          row.names=F)

```

Stats

```{r}

#Make a new dataframe to store average correlations across runs
plotCors <- trialCors[trialCors$o==1, ]

#Rearrange to fit the order output by the upcoming tapply()
plotCors <- plotCors[order(plotCors$method,plotCors$trial),]

#Compute average performance across all model loops.
plotCors$r <- c(tapply(trialCors$r,
                       list(trialCors$trial,
                            trialCors$method),
                       mean))

# plot 
corPlot<-ggplot(plotCors, mapping = aes(x = trial, y = r, color = method))+
  geom_line() +
  labs(x = "Adjustment Trials",
       y = expression(italic("r")["Learned, Real Preferences"])) +
  scale_color_discrete(name = "Adjustment Method") +
  theme_grey(base_size = 13.5)
#, labels = c("Imp-Constant", "Random-Constant", "All-Imp", "Imp-Imp", "Random-Imp", "All-Prob&Imp", "Random-Random"))  



#Do the same thing for sex differences

#Create a new dataframe for storing average learned preferences across runs
prefAvs<-prefData[prefData$o==1,]
prefAvs<-prefAvs[order(prefAvs$method,prefAvs$PIN),]


#Compute the average learned preferences across model runs
prefMeans<-sapply(colnames(prefAvs[40:53]),function(x)
  c(tapply(prefData[,colnames(prefData)==x],
           list(prefData$PIN,prefData$method),
           mean))
  )

#Save these to the datarame
prefAvs[,40:53]<-prefMeans

#Compare sex differences in attractiveness, resources, and kindness
tPhysatt <- t.test(learned_physatt ~ sex,
                   data = prefAvs[prefAvs$method == "allImp", ])

tRes <- t.test(learned_resources ~ sex,
               data = prefAvs[prefAvs$method == "allImp", ])

tKind <- t.test(learned_kind ~ sex,
                data = prefAvs[prefAvs$method == "allImp", ])



#Melt average preference data for plotting
prefMelt<-melt(as.data.table(prefAvs[prefAvs$method=="allImp",]),
               id.vars=1:9,
               measure.vars=c(40:53))

#Convert sex to a factor
prefMelt$sex<-factor(prefMelt$sex,labels=c("Female","Male"))

#Plot density plots of preferences
prefPlot <- ggplot(prefMelt,
                   aes(y = variable,
                       x = value,
                       fill = sex)) +
  geom_density_ridges(alpha=.5) +
  labs(x = "Learned Preference", y = "Trait") +
  scale_fill_discrete(name = "Participant Sex") +
  theme_grey(base_size = 25) +
  scale_y_discrete(labels = tools::toTitleCase(gsub(
    "learned_", "", unique(prefMelt$variable)
  )))
```
